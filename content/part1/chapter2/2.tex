Despite the fact that compilers are used to translate programs from one form to another, they can also be considered large software systems that use various algorithms and data structures. The knowledge obtained by studying compilers can be used to design other scalable software systems as well. On the other hand, compilers are also a subject of active scientific research, and there are many unexplored areas and topics to investigate.

You can find some basic information about the internal structure of a compiler here. We will keep it as basic as possible so the information applies to any compiler, not just Clang. We will briefly cover all phases of compilation, which will help to understand Clang's position in the overall compiler architecture.

\mySubsubsection{2.2.1.}{Exploring the compiler workflow}

The primary function of a compiler is to convert a program written in a specific programming language (such as C/C++ or FORTRAN) into a format that can be executed on a target platform. This process involves the use of a compiler, which takes the source file and any compilation flags, and produces a build artifact, such as an executable or object file, as shown in Figure 2.1.

\myGraphic{0.4}{content/part1/chapter2/images/1.png}{Figure 2.1: Compiler workflow}

The term "target platform" can have a broad meaning. It can refer to machine code that is executed on the same host, as is typically the case. But it can also refer to cross-compilation, where the compiler generates code for a different computer architecture than the host. For example, code for a mobile application or embedded application running on ARM can be generated using an Intel machine as the host. Additionally, the target platform is not limited to machine code only. For example, some early C++ compilers (such as "cc") would produce pure C code as output. This was done because, at the time, C was the most widely used and well-established programming language, and the C compiler was the most reliable way to generate machine code. This approach allowed early C++ programs to be run on a wide range of platforms since most systems already had a C compiler available. The produced C code could then be compiled into machine code using any popular C Compiler such as GCC or LCC.

\myGraphic{0.4}{content/part1/chapter2/images/2.png}{Figure 2.2: Typical compiler workflow: source program is passed via different stages: frontend, middle-end, and backend}

We are going to focus on compilers that produce binary code, and a typical compiler workflow for such a compiler is shown in Figure 2.2. The stages of compilation can be described as follows:

\begin{itemize}
\item
Frontend: The frontend does lexical analysis and parsing, which includes both syntax analysis and semantic analysis. The syntax analysis assumes that your program is well-organized according to the language grammar rules. The semantic analysis performs checks on the program's meaning and rejects invalid programs, such as those that use wrong types.

\item
Middle-end: The middle-end performs various optimizations on the intermediate representation (IR) code (LLVM-IR for Clang).

\item
Backend: The Backend of a compiler takes the optimized or transformed IR and generates machine code or assembly code that can be executed by the target platform.
\end{itemize}

The source program is transformed into different forms as it passes through the various stages. For example, the frontend produces IR code, which is then optimized by the middle-end, and finally converted into native code by the backend (see Figure 2.3).

\myGraphic{0.4}{content/part1/chapter2/images/3.png}{Figure 2.3: Source code transformation by compiler}

Input data consists of Source code and Compile options. The source code is transformed by the Frontend into IR. The Middle-end does different optimizations on IR and passes the final (optimized) result to the Backend. The Backend generates the Target code. The Frontend, Middle-end, and Backend use Compile options as settings for the code transformations. Let's look into the compiler frontend as the first component of the compiler's workflow.

\mySubsubsection{2.2.2.}{Frontend}

The primary goal for the frontend is to convert a given source code to intermediate form. It's worth mentioning that the frontend also transforms the source code into various forms before it produces the IR. The frontend will be our primary focus in the book, so we will examine its components. The first component of the

frontend is the Lexer (see Figure 2.4). It converts the source code into a set of tokens, which are used to create a special data structure called the abstract syntax tree (AST ). The final component, the code generator (Codegen), traverses the AST and generates the IR from it.

\myGraphic{0.4}{content/part1/chapter2/images/4.png}{Figure 2.4: Compiler frontend}

The source code is transformed into a set of tokens (Toks) by the Lexer . The Parser takes the tokens and creates an Abstract Syntax Tree (AST ) that we will explore in details later in Chapter 3, Clang AST. The Codegen generates IR from the AST .

We will use a simple C/C++ program that calculates the maximum of two numbers to demonstrate the workings of the frontend. The code for the program is as follows:

\begin{cpp}
int max(int a, int b) {
  if (a > b)
    return a;
  return b;
}
\end{cpp}

\begin{center}
Figure 2.5: Test program for compiler frontend
\end{center}

The first component of the frontend is the lexer. Let's examine it.

\mySamllsection{Lexer}

The frontend process starts with the Lexer , which converts the input source into a stream of tokens. In our example program (see Figure 2.5), the first token is the keyword int , which represents the integer type. This is followed by the identifier max for the function name. The next token is the left parenthesis ( , and so on (see Figure 2.6).

\myGraphic{0.4}{content/part1/chapter2/images/6.png}{Figure 2.6: Lexer : the program source is converted into a stream of tokens}

\mySamllsection{Parser}

The Parser is the next component following the Lexer . The primary output produced by the Parser is called an abstract syntax tree (AST). This tree represents the abstract syntactic structure of the source code written in a programming language. The Parser generates the AST by taking the stream of tokens produced by the Lexer as input and organizing them into a tree-like structure. Each node in the tree represents a construct in the source code, such as a statement or expression, and the edges between nodes represent the relationships between these constructs.

\myGraphic{0.4}{content/part1/chapter2/images/7.png}{Figure 2.7: The AST for our example program, which calculates a maximum of two numbers}

The AST for our example program is shown in Figure 2.7. As you can see, our function (max ) has two parameters (a and b ) and a body. The body is marked as a compound statement in Figure 2.7, see also Figure 2.40, where we provide a definition for a compound statement from the C++ standard. The compound statement consists of other statements, such as return and if . The a and b variables are used in the bodies of these statements. You may also be interested in the real AST generated by Clang for the compound statement, the result of which is shown in Figure 2.8.

\myGraphic{0.4}{content/part1/chapter2/images/8.png}{Figure 2.8: The AST for the compound statement generated by Clang . The tree generated by the clang -cc1 -ast-view <...> command}

The Parser performs two activities:

\begin{enumerate}
\item
Syntax analysis: the Parser constructs the AST by analyzing the syntax of the program.

\item
Semantic analysis: the Parser analyzes the program semantically.
\end{enumerate}

One of the jobs of the parser is to produce an error message if the parsing fails in either of the syntax or semantic analysis phases. If no error occurs, then we get a parse tree (or an AST) for the syntax analysis and a semantically verified parse tree in the case of semantic analysis. We can get a sense of this by considering what types of errors are detected by syntax analysis and which ones are detected by semantic analysis.

Syntax analysis assumes that the program should be correct in terms of the grammar specified for the language. For example, the following program is invalid in terms of syntax because a semicolon is missing from the last return statement:

\begin{cpp}
int max(int a, int b) {
  if (a > b)
    return a;
  return b // missing ;
}
\end{cpp}

\begin{center}
Figure 2.9: Listing of program code with a syntax error
\end{center}

Clang produces the following output for the program:

\begin{shell}
max_invalid_syntax.cpp:4:11: error: expected ';' after return statement
  return b // missing ;
          ^
          ;
\end{shell}

\begin{center}
Figure 2.10: Compiler output for a program with a syntax error
\end{center}

On the other hand, a program can be syntactically correct but make no sense. The Parser should detect a semantic error in such cases. For instance, the following program has a semantic error related to the wrongly used type for the return value:

\begin{cpp}
int max(int a, int b) {
  if (a > b)
    return a;
  return &b; // invalid return type
}
\end{cpp}

\begin{center}
Figure 2.11: Listing of program code with a semantic error
\end{center}

Clang generates the following output for the program:

\begin{shell}
max_invalid_sema.cpp:4:10: error: cannot initialize return object of type \
'int' with an rvalue of type 'int *'
  return &b; // invalid return type
         ^~
\end{shell}

\begin{center}
Figure 2.12: Compiler output for a program with a semantic error
\end{center}

AST is mainly constructed as a result of syntax analysis, but for certain languages, such as C++, semantic analysis is also crucial for constructing the AST, particularly for C++ template instantiation.

During syntax analysis, the compiler verifies that the template declaration adheres to the language's grammar and syntax rules, including the proper use of keywords such as "template" and "typename," as well as the formation of the template parameters and body.

Semantic analysis, on the other hand, involves the compiler performing template instantiation, which generates the AST for specific instances of the template. It's worth noting that the semantic analysis of templates can be quite complex, as the compiler must perform tasks such as type checking, name resolution, and more for each template instantiation. Additionally, the instantiation process can be recursive and lead to a significant amount of code duplication, known as code bloat. To combat this, C++ compilers employ techniques such as template instantiation caching to minimize the amount of redundant code generated.

\mySamllsection{The codegen}

The codegen (it's worth mentioning that we also have another Codegen component as a part of Backend that generate the target code) or code generator, which is the final component of the compiler's frontend, has the primary goal of generating the Intermediate Representation (IR). For this purpose, the compiler traverses the AST generated by the parser and converts it into other source code that is called the Intermediate Representation or IR. The IR is a language-independent representation, allowing the same middle-end component to be used for different frontends (FORTRAN vs C++). Another reason for using an Intermediate Representation (IR) is that if we have a new architecture available tomorrow, we can generate the target code specific to that architecture. Since the source language remains unchanged, all the steps leading up to the IR will remain the same. The IR provides this flexibility.

The use of IRs in compilers is a concept that has been around for several decades. The idea of using an intermediate representation to represent the source code of a program during compilation has evolved over time, and the exact date when IR was first introduced in compilers is not clear.

However, it is known that the first compilers in the 1950s and 1960s did not use IRs and instead translated source code directly into machine code. By the 1960s and 1970s, researchers had begun experimenting with using IRs in compilers to improve the efficiency and flexibility of the compilation process.

One of the first widely used IRs was three-address code, which was used in the mid-1960s in IBM/360's FORTRAN compiler. Other early examples of IRs include the register transfer language (RTL) and the static single assignment (SSA) form, which were introduced in the 1970s and 1980s respectively.

Today, the use of IRs in compilers is a standard practice, and many compilers use multiple IRs throughout the compilation process. This allows for more powerful optimization and code generation techniques to be applied.




































